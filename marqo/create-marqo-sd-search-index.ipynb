{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19e09e3-96cd-42b6-a57e-b4122a62d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/marqo-ai/marqo\n",
    "# https://marqo.pages.dev/End-to-End%20Examples/streamlit_demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b4662-db9f-4811-8f46-92fea38d4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the marqo docker container to read files from local os. For more info on this please visit this link.\n",
    "# https://github.com/marqo-ai/marqo/issues/35\n",
    "# python3 -m http.server 8222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29c60e4-4210-4202-8157-7672dfc4be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo chmod 666 /var/run/docker.sock\n",
    "# sudo docker rm -f marqo;docker run --name marqo -it --privileged -p 8882:8882 --add-host host.docker.internal:host-gateway marqoai/marqo:0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c39a5f-840d-489a-b671-17a0c41eab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb\n",
    "!pip install marqo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dad3c08-2499-4165-9836-acf32f70ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma-sh0ts\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427b633a-a3d3-49e6-af2e-f2b368ef797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"krea-open-prompts\"\n",
    "run_name = \"insert-open-prompts-sd-marqo\"\n",
    "tags = [\"download\", \"stable_diffusion\", \"insert\", \"marqo\"]\n",
    "_config = {\n",
    "    # option of \"prompts\" or \"sample_prompts\"\n",
    "    \"dataset\": \"sample_prompts\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e23ab8a-1544-499b-b218-c41343d2ced0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/krea-marqo/wandb/run-20220927_140003-28g33g7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/a-sh0ts/krea-open-prompts/runs/28g33g7p\" target=\"_blank\">insert-open-prompts-sd-marqo</a></strong> to <a href=\"https://wandb.ai/a-sh0ts/krea-open-prompts\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=project_name, name=run_name, tags=tags, config=_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d435bf-3346-4ce7-8e7a-1fed72217427",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = run.config\n",
    "dataset_name = config['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fabef9-1053-484e-81ce-339f4d81f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact sample_prompts:latest, 618.73MB. 997 files... Done. 0:0:0.2\n"
     ]
    }
   ],
   "source": [
    "dataset_path = run.use_artifact(f'{dataset_name}:latest', type='processed_data').download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793863db-8f4a-4a08-a37c-dd737be9e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e299bb2-1d2e-4639-a71e-e72dd58bc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = Path(dataset_path, f\"{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a04b20b-7935-42bf-a6e8-52199926c0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "# from modin.config import ProgressBar\n",
    "# ProgressBar.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbcb0d9-416b-46a1-abc5-e5c6ed32a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2362116-74c4-4cca-bb0a-1dad0e17ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: better way to prepend artifact path\n",
    "df[\"local_image_location\"] = str(dataset_path) + \"/\" + df[\"local_image_location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "825cc5b1-a881-44d8-8265-9e4438dcdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional prepend for marqo\n",
    "df[\"local_image_location\"] = \"http://host.docker.internal:8222/\" + df[\"local_image_location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79baf54e-8753-4612-9ae1-a244d3b13cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://host.docker.internal:8222/./artifacts/sample_prompts:v2/sample_prompts/A_portrait_photo_of_a_kangaroo_wearing_an_orange_hoodie_and_blue_sunglasses_standing_on_the_grass_in_front_of_the_Sydney_Opera_House_holding_a_sign_-C_15.0_-n_9_-i_-S_556046175_ts-1660001285_idx-4.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"local_image_location\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89004251-49ad-4e3e-b30f-49a56467dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e38449-21b6-4d34-91f2-f10d980c849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUG: Using tradiitonla pandas to flatten the dataframe before distributing in modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b75c27c9-2e48-4a31-aeee-e1554b2e4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884e7ae1-0311-434c-8978-75a70503474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marqo import Client\n",
    "from marqo.errors import MarqoApiError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0ebf936-643b-43ce-b511-83e0e483a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "marqo_settings = {\n",
    "    \"index_defaults\": {\n",
    "        \"treat_urls_and_pointers_as_images\": True,\n",
    "        \"image_preprocessing\": {\n",
    "            \"patch_method\": \"frcnn\"\n",
    "        },\n",
    "        \"model\":\"ViT-B/16\",\n",
    "        \"normalize_embeddings\":True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77e86ba0-9dba-4ce2-860c-9afb1f68b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "mq = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db31dd47-54d9-468f-97ed-53d2bebd94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"stable-diffusion-open-prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae9412e2-42fb-4ef1-a153-2eecce03ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has been killed and brought back anew sire. Thirsty for knowledge\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mq.create_index(index_name, settings_dict=marqo_settings)\n",
    "    print(\"Risen from nothing, it wishes to learn\")\n",
    "except:\n",
    "    mq.index(index_name).delete()\n",
    "    mq.create_index(index_name, settings_dict=marqo_settings)\n",
    "    print(\"It has been killed and brought back anew sire. Thirsty for knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89e6af00-fe66-465d-8045-b78e09253021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ee1345d-521e-491d-a84f-adb12a0289b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #TODO: Add lightweight logic to see which gpu devices are available\n",
    "NUM_CPUS = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32394a81-958f-4f15-9249-5410154a457a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we use parallel indexing to speed up the task\n",
    "responses = mq.index(index_name).add_documents(data, device=device, processes=NUM_CPUS, batch_size=50)\n",
    "# for data_doc in tqdm(data):\n",
    "#     try:\n",
    "#         responses = mq.index(index_name).add_documents([data_doc], device=device)\n",
    "#         # print(f\"<SUCCESS>\\nAdded prompt:\\n{data_doc['prompt']}\\nURI: {data_doc['local_image_location']}\\n\")\n",
    "#     except:\n",
    "#         print(f\"<FAILURE>\\nSkipping prompt:\\n{data_doc['prompt']}\\nURI: {data_doc['local_image_location']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d80332b8-3fbb-4bbc-aad7-10eee9582f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split into two notebooks\n",
    "# First is half of this -> prep data for insertion\n",
    "# That includes the transforms done here\n",
    "# instead of ignoring the problematic opened images, let us pre-download all the images and place it alongside tha appropriate dataset\n",
    "#     and have the image location be the name of the file where it will prepended with the artifact location after downlaoded\n",
    "# If it is rate limit add some logic to check the files in an artifact against the table of downloaded files already\n",
    "# and download that which was not downloaded and relog with that which was alrewady logged?\n",
    "# Second will be the insertion which will simply pull the appropriate data down and log"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
